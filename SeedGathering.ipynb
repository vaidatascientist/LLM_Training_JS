{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7093411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhavgupta/Desktop/LLM_Training_JS/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "data = load_from_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171ab48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'sha1', 'id'],\n",
       "    num_rows: 17581\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e9d4c-f390-4aba-9ec8-627b9ed85aea",
   "metadata": {},
   "source": [
    "### SEED GATHERING GET CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd3e144-73cb-42fa-9550-075c51686a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_javascript as tsjs\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "LANGUAGE = Language(tsjs.language())\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "import signal\n",
    "from multiprocessing import Pool\n",
    "import boto3\n",
    "import smart_open\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6df72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up S3 client\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "def download_contents(blob_id, src_encoding):\n",
    "    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n",
    "    with smart_open.open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "        content = fin.read().decode(src_encoding)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01581783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JavaScript-specific Tree-sitter query for top-level functions with docstrings\n",
    "TOPLEVEL_DOCSTRING_QUERY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "  (function_declaration\n",
    "    name: (identifier)\n",
    "    body: (statement_block . \n",
    "      (comment) @docstring)) @function.def\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "def node_to_string(buf, node):\n",
    "    return buf[node.start_byte:node.end_byte].decode(\"utf8\", errors=\"ignore\")\n",
    "\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOCSTRING_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    for node, ty in captures:\n",
    "        if ty != \"function.def\":\n",
    "            continue\n",
    "        _, col = node.start_point\n",
    "        if col != 0:\n",
    "            continue\n",
    "        res.append(node_to_string(src, node))\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_ex(parser, ex):\n",
    "    try:\n",
    "        ex = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_fns_with_docstrings(buf, tree)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing blob {ex.get('blob_id')}: {e}\")\n",
    "        return []\n",
    "\n",
    "PARSERS = None\n",
    "\n",
    "def process_chunk(idx_and_chunk):\n",
    "    global PARSERS\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    for ex in chunk:\n",
    "        chunk_new_funs.update(parse_ex(parser, ex))\n",
    "    return chunk_new_funs\n",
    "\n",
    "def make_parser():\n",
    "    parser = Parser(LANGUAGE)\n",
    "    return parser\n",
    "\n",
    "def main_js():\n",
    "    global PARSERS\n",
    "    ds = datasets.load_dataset(\n",
    "        \"bigcode/the-stack-v2-dedup\",\n",
    "        \"JavaScript\",\n",
    "        streaming=True,\n",
    "        split=\"train\",\n",
    "        cache_dir=\"./stack_cache\"\n",
    "    )\n",
    "\n",
    "    funs = set()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    PARSERS = [make_parser() for _ in range(NUM_WORKERS)]\n",
    "    CHUNK_SIZE = 1000 * NUM_WORKERS\n",
    "\n",
    "    chunk = []\n",
    "    p = Pool(NUM_WORKERS)\n",
    "\n",
    "    i = 0\n",
    "    for ex in ds:\n",
    "        try:\n",
    "            chunk.append(ex)\n",
    "            if len(chunk) == CHUNK_SIZE:\n",
    "                print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "                subchunk_size = len(chunk) // NUM_WORKERS\n",
    "                subchunks = [chunk[j:j + subchunk_size] for j in range(0, len(chunk), subchunk_size)]\n",
    "                new_funs_iter = p.imap(process_chunk, [(j, subchunk) for j, subchunk in enumerate(subchunks)])\n",
    "\n",
    "                print(\"Getting new functions\")\n",
    "                len_before = len(funs)\n",
    "\n",
    "                while True:\n",
    "                    try:\n",
    "                        def timeout_handler(_, __):\n",
    "                            raise KeyboardInterrupt\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(60)\n",
    "                        funs.update(next(new_funs_iter))\n",
    "                        signal.alarm(0)\n",
    "                    except KeyboardInterrupt:\n",
    "                        signal.alarm(0)\n",
    "                        print(\"Timeout or keyboard interrupt. Restarting pool.\")\n",
    "                        p.terminate()\n",
    "                        p.join()\n",
    "                        p = Pool(NUM_WORKERS)\n",
    "                        break\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during pool iteration: {e}\")\n",
    "\n",
    "                PARSERS = [make_parser() for _ in range(NUM_WORKERS)]\n",
    "                print(f\"✅ Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions.\")\n",
    "                chunk = []\n",
    "\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Progress: {i} examples processed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on example {i}: {e}\")\n",
    "            chunk = []\n",
    "\n",
    "        # Optional: early stop if needed\n",
    "        # if i > 10000:\n",
    "        #     break\n",
    "\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    print(f\"✅ Total JavaScript functions extracted: {len(funs)}\")\n",
    "    return funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1f885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ca3c0ff49148cdbb20118ac41aa92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000 examples processed.\n",
      "Progress: 2000 examples processed.\n",
      "Progress: 3000 examples processed.\n",
      "Progress: 4000 examples processed.\n",
      "Progress: 5000 examples processed.\n",
      "Progress: 6000 examples processed.\n",
      "Progress: 7000 examples processed.\n",
      "Processing chunk 0\n",
      "Getting new functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout or keyboard interrupt. Restarting pool.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-13:\n",
      "Process SpawnPoolWorker-9:\n",
      "Process SpawnPoolWorker-15:\n",
      "Process SpawnPoolWorker-10:\n",
      "Process SpawnPoolWorker-14:\n",
      "Process SpawnPoolWorker-16:\n",
      "Process SpawnPoolWorker-12:\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done processing chunk 0. Got 0 new functions.\n",
      "Progress: 8000 examples processed.\n",
      "Progress: 9000 examples processed.\n",
      "Progress: 10000 examples processed.\n",
      "Progress: 11000 examples processed.\n",
      "Progress: 12000 examples processed.\n",
      "Progress: 13000 examples processed.\n",
      "Progress: 14000 examples processed.\n",
      "Progress: 15000 examples processed.\n",
      "Processing chunk 1\n",
      "Getting new functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "all_functions = main_js()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = datasets.Dataset.from_dict({\n",
    "    \"content\": list(all_functions),\n",
    "    \"id\": list(range(len(all_functions)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d75d7-bac9-4f47-bb2c-a5773f46ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = new_ds\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa60b0-bf47-4b30-93e2-2ee8a71958bf",
   "metadata": {},
   "source": [
    "### SEED GATHERING HIGH-QUALITY SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1fe7e6-04d4-49c6-b19f-3ebf3d7642e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import signal\n",
    "import hashlib\n",
    "import os\n",
    "import argparse\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from tree_sitter_parser import LANGUAGE, global_parser\n",
    "\n",
    "RETURN_QUERY = LANGUAGE.query(\"\"\"\n",
    "(return_statement) @return\n",
    "\"\"\")\n",
    "\n",
    "def does_have_return(src):\n",
    "    tree = global_parser.parse(bytes(src, \"utf8\"))\n",
    "    root = tree.root_node\n",
    "    captures = RETURN_QUERY.captures(root)\n",
    "    for node, _ in captures:\n",
    "        # if it doesn't have an argument, it's not a return with a value\n",
    "        if len(node.children) <= 1:  # includes \"return\" itself\n",
    "            continue\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# runs mypy in the given directory, returns stdout\n",
    "# then, it logs the number of errors for each file\n",
    "def run_mypy(d):\n",
    "    try:\n",
    "        outs = subprocess.run(\n",
    "            [\"mypy\", \".\"],\n",
    "            cwd=d,\n",
    "            capture_output=True,\n",
    "            timeout=120,\n",
    "            text=True,\n",
    "        ).stdout\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    filemap = {}\n",
    "    lines = outs.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) >= 2:\n",
    "                file = parts[0].split(\"/\")[-1]\n",
    "                if file not in filemap:\n",
    "                    filemap[file] = 0\n",
    "                if \"error:\" in line:\n",
    "                    filemap[file] += 1\n",
    "\n",
    "    return filemap\n",
    "\n",
    "def typecheck_batch(files: List[str]) -> Dict[str, str]:\n",
    "    # Create a temporary directory using the tempfile module\n",
    "    filemap: Dict[str, str] = {}\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        for contents in files:\n",
    "            hash_object = hashlib.sha1(bytes(contents, \"utf8\"))\n",
    "            hex_dig = hash_object.hexdigest()\n",
    "            filemap[hex_dig] = contents\n",
    "            name = os.path.join(tempdir, hex_dig + \".py\")\n",
    "            with open(name, \"w\") as f:\n",
    "                f.write(contents)\n",
    "\n",
    "        # Run mypy in the temporary directory\n",
    "        typecheck_map = run_mypy(tempdir)\n",
    "        print(typecheck_map)\n",
    "\n",
    "        if typecheck_map is None:\n",
    "            return {}\n",
    "\n",
    "        for contents, errors in typecheck_map.items():\n",
    "            no_py = contents.replace(\".py\", \"\")\n",
    "            if errors == 0:\n",
    "                continue\n",
    "            if no_py in filemap:\n",
    "                del filemap[no_py]\n",
    "\n",
    "        print(f\"Pass rate: {len(filemap)}/{len(files)}\")\n",
    "        return filemap\n",
    "\n",
    "def infer_imports(code: str) -> str:\n",
    "    import autoimport\n",
    "    try:\n",
    "        def handler(signum, frame):\n",
    "            raise Exception(\"Timeout\")\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(10)\n",
    "        inferred = autoimport.fix_code(code)\n",
    "        signal.alarm(0)\n",
    "        return inferred\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)\n",
    "        print(f\"Error while inferring imports: {e}\")\n",
    "        return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5889ead-f07b-460e-8798-37df9b0c8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtering to only functions with return statements\")\n",
    "ds = ds.filter(lambda ex: does_have_return(\n",
    "    ex[\"content\"]), num_proc=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d780ab-b991-46e4-b542-8b8802734c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3fd22-9743-45e8-b17a-2195a5119c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.infer_imports:\n",
    "#     print(\"Inferring imports for functions\")\n",
    "#     ds = ds.map(lambda ex: {\"content\": infer_imports(\n",
    "#         ex[\"content\"])}, num_proc=os.cpu_count())\n",
    "\n",
    "batch = []\n",
    "max_i = len(ds) - 1\n",
    "\n",
    "new_ds = {\n",
    "    \"content\": [],\n",
    "    \"sha1\": [],\n",
    "    \"id\": [],\n",
    "}\n",
    "\n",
    "e_id = 0\n",
    "\n",
    "for i, ex in enumerate(tqdm(ds, total=len(ds))):\n",
    "    try:\n",
    "        code = ex[\"content\"]\n",
    "\n",
    "        batch.append(code)\n",
    "\n",
    "        if len(batch) == 250 or i == max_i:\n",
    "            filemap = typecheck_batch(batch)\n",
    "            for sha1, contents in filemap.items():\n",
    "                new_ds[\"content\"].append(contents)\n",
    "                new_ds[\"sha1\"].append(sha1)\n",
    "                new_ds[\"id\"].append(e_id)\n",
    "                e_id += 1\n",
    "            batch = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"There was an error: {e}\")\n",
    "        continue\n",
    "\n",
    "new_ds_hf = datasets.Dataset.from_dict(new_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c04146-d49d-4cfd-83f9-0aa270ea3ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d692b2-f8be-48d9-9e75-b07ee1c422f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7cfd7-af58-4f7e-b88a-616014b4f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_ds_hf['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418df388-25a5-460d-825b-c95ede49b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../datasets/seed2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673ee98-d98e-4766-931c-2a921293fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_hf.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6a4e5-2734-4602-80c9-5391521b391e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53c67802-a173-4df2-b661-234745f0bfdf",
   "metadata": {},
   "source": [
    "### SEED GATHERING FILTER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01086352-e230-4267-9b7b-feab665f681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "from tree_sitter_parser import global_parser, LANGUAGE, does_have_return, make_parser\n",
    "import benchmark_data\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "from vllm import LLM, SamplingParams\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfc94c-0964-4eaa-9e9c-7749bb4cf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_BLOCK_QUERY = LANGUAGE.query(\"\"\"\n",
    "(function_definition\n",
    "  body: (block) @fn-block)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def template_few_shot(code, answer, rationale):\n",
    "    doc, code = js_extract_docstring(code)  # Update to extract JavaScript comments\n",
    "    prompt = f\"\"\"\n",
    "<issue_start>username_0: I have a function in JavaScript and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good comment for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```js\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is: {answer}\n",
    "\n",
    "{rationale}\n",
    "\n",
    "Upvotes: 200\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "FEW_SHOTS = [\n",
    "    (\n",
    "        '''def simple_scan_network():\n",
    "    \"\"\"\n",
    "    Do a simple network scan, which only works if your network configuration\n",
    "    is 192.168.1.x\n",
    "    \"\"\"\n",
    "    base_ip = \"192.168.1.\"\n",
    "    addresses = ['127.0.0.1']\n",
    "\n",
    "    for index in range(1, 255):\n",
    "        addresses.extend([base_ip + str(index)])\n",
    "\n",
    "    return addresses''',\n",
    "        \"No\",\n",
    "        \"The simple_scan_network function you have provided seems to generate addresses that then would be used for a network scan, but does not actually perform it, unlike the function claims.\",\n",
    "    ),\n",
    "    (\n",
    "        '''import pandas\n",
    "\n",
    "\n",
    "def coerce_integer(df):\n",
    "    \"\"\"\n",
    "    Loop through the columns of a df, if it is numeric,\n",
    "    convert it to integer and fill nans with zeros.\n",
    "    This is somewhat heavy-handed in an attempt to force\n",
    "    Esri to recognize sparse columns as integers.\n",
    "    \"\"\"\n",
    "    # Numeric columns to not coerce to integer\n",
    "    EXCEPT = [\"latitude\", \"longitude\", \"zipCode\"]\n",
    "\n",
    "    def numeric_column_to_int(series):\n",
    "        return (\n",
    "            series.fillna(0).astype(int)\n",
    "            if pandas.api.types.is_numeric_dtype(series) and series.name not in EXCEPT\n",
    "            else series\n",
    "        )\n",
    "\n",
    "    return df.transform(numeric_column_to_int, axis=0)''',\n",
    "        \"Yes\",\n",
    "        \"The docstring does seem to match the implementation! The function loops through the columns of a df and coerces it as explained.\",\n",
    "    ),\n",
    "    ('''def __trans_df_into_dict(data):\n",
    "    \"\"\"Converte DataFrame to dictionary.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        dict: Name dictionary.\n",
    "    \"\"\"\n",
    "    data[\"en_name\"] = data[\"en_name\"].str.upper()\n",
    "    data[\"en_name_f\"] = data[\"en_name\"].str.split(\" \", expand=True)[0]\n",
    "    data[\"en_name_l\"] = data[\"en_name\"].str.split(\" \", expand=True)[1]\n",
    "    data[\"jp_name_f\"] = data[\"jp_name\"].str.split(\"・\", expand=True)[0]\n",
    "    data[\"jp_name_l\"] = data[\"jp_name\"].str.split(\"・\", expand=True)[1]\n",
    "    fullname_dict = dict(zip(data[\"en_name\"], data[\"jp_name\"]))\n",
    "    fname_dict = dict(zip(data[\"en_name_f\"], data[\"jp_name_f\"]))\n",
    "    lname_dict = dict(zip(data[\"en_name_l\"], data[\"jp_name_l\"]))\n",
    "    return fullname_dict, fname_dict, lname_dict''',\n",
    "     \"No\",\n",
    "     \"The function__trans_df_into_dict  does indeed convert a dataframe into a dictionary, however, it converts various columns that were not described in the docstring.\\nFor instance, nowhere in the docstring it mentions handling japanese characters or the name of the column.\",\n",
    "     ),\n",
    "    (\n",
    "        '''def inchesToMeters(inches):\n",
    "    \"\"\"Convert inches to meters.\"\"\"\n",
    "    return inches * 0.0254''',\n",
    "        \"Yes\",\n",
    "        \"inchesToMeters is a very simple function, the doccstring explains concisely its purpose, which is of converting inches to meters.\",\n",
    "    ),\n",
    "    ('''def square_crop(im, target_size=None):\n",
    "  \"\"\" Crop image to `target_size`. If that's None the image is squared\n",
    "  to the smallest size\n",
    "  \"\"\"\n",
    "\n",
    "  w = im.size[0]\n",
    "  h = im.size[1]\n",
    "\n",
    "  target_size = target_size if target_size else min(w, h)\n",
    "\n",
    "  dx = (w - target_size) / 2\n",
    "  dy = (h - target_size) / 2\n",
    "\n",
    "  return im.crop((dx, dy, dx + target_size, dy + target_size))''',\n",
    "     \"Yes\",\n",
    "     \"Following the standard description for docstrings for functions and methods, the square_crop function description tells exactly what the function does.\"\n",
    "     ),\n",
    "    ('''def _setup_motifs_files(args):\n",
    "    \"\"\"convenience fn, make sure setup is same across\n",
    "    multiplicity/orientation/spacing workflows\n",
    "    \"\"\"\n",
    "    motifs_files = {}\n",
    "    motifs_files[\"early\"] = \"{}/{}/ggr.scanmotifs.h5\".format(\n",
    "        args.inputs[\"inference\"][args.cluster][\"scanmotifs_dir\"],\n",
    "        args.inputs[\"inference\"][args.cluster][\"scanmotifs_early_dir\"])\n",
    "    motifs_files[\"mid\"] = \"{}/{}/ggr.scanmotifs.h5\".format(\n",
    "        args.inputs[\"inference\"][args.cluster][\"scanmotifs_dir\"],\n",
    "        args.inputs[\"inference\"][args.cluster][\"scanmotifs_mid_dir\"])\n",
    "    motifs_files[\"late\"] = \"{}/{}/ggr.scanmotifs.h5\".format(\n",
    "        args.inputs[\"inference\"][args.cluster][\"scanmotifs_dir\"],\n",
    "        args.inputs[\"inference\"][args.cluster][\"scanmotifs_late_dir\"])\n",
    "\n",
    "    return motifs_files''',\n",
    "     \"No\",\n",
    "     \"The docstring for _setup_motifs_files just says this is a convenience function. There is definitely not enough information to re-implement this function from the docstring alone.\",\n",
    "     ),\n",
    "    ('''def trip(u, v):\n",
    "    \"\"\"\n",
    "    Returns the scalar triple product of vectors u and v and z axis.\n",
    "    The convention is z dot (u cross v). Dotting with the z axis simplifies\n",
    "    it to the z component of the u cross v\n",
    "    The product is:\n",
    "        positive if v is to the left of u, that is,\n",
    "          the shortest right hand rotation from u to v is ccw\n",
    "        negative if v is to the right of u, that is,\n",
    "          the shortest right hand rotation from u to v is cw\n",
    "        zero if v is colinear with u\n",
    "    Essentially trip is the z component of the cross product of u x v\n",
    "    \"\"\"\n",
    "    return (u[0] * v[1] - u[1] * v[0])''',\n",
    "     \"Yes\",\n",
    "     \"The docstring for the trip function is very detailed and describes the function's purpose and the mathematical formula used to calculate the scalar triple product.\",\n",
    "     )\n",
    "]\n",
    "\n",
    "\n",
    "def prompt_fmt(code):\n",
    "    doc, code = py_extract_docstring(code)\n",
    "    random.shuffle(FEW_SHOTS)\n",
    "    buf = \"\"\n",
    "    for few in FEW_SHOTS:\n",
    "        buf += template_few_shot(*few)\n",
    "    buf += f\"\"\"<issue_start>username_0: I have a function in Python and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```py\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.\n",
    "Upvotes: 100<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is:\"\"\"\n",
    "    return buf\n",
    "\n",
    "\n",
    "def auto_dtype():\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        return \"bfloat16\"\n",
    "    return \"auto\"\n",
    "\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    chunks = []\n",
    "    for i in range(0, len(lst), n):\n",
    "        chunk = []\n",
    "        for j in range(n):\n",
    "            if i + j < len(lst):\n",
    "                chunk.append(lst[i + j])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfa4b2-d3bb-4560-9578-8ce15d4d68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = new_ds_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0992371-9ad5-4ad9-8aaa-d74b25eb4da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(dataset)} examples. Running pre-filtering...\")\n",
    "\n",
    "BAD_WORDS = [\"todo\", \"fixme\", \"bug\"]\n",
    "BAD_IMPORTS = [\"argparse\", \"os\", \"subprocess\", \"sys\", \"setuptools\",\n",
    "               \"distutils\", \"matplotlib\", \"seaborn\"]\n",
    "BAD_IMPORTS = [f\"import {b}\" for b in BAD_IMPORTS] + \\\n",
    "    [f\"from {b}\" for b in BAD_IMPORTS]\n",
    "BAD_SUBSTRINGS = BAD_WORDS + BAD_IMPORTS\n",
    "\n",
    "bench_filter = benchmark_data.filter_out()\n",
    "all_bench = bench_filter[\"human_eval_docstrings\"] + \\\n",
    "    bench_filter[\"human_eval_solutions\"] + \\\n",
    "    bench_filter[\"mbpp_docstrings\"] + \\\n",
    "    bench_filter[\"mbpp_solutions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af077da-feac-4358-8702-e3c06927f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_filtering(ex):\n",
    "    code = ex[\"content\"]\n",
    "    code_bytes = code.encode('utf-8')\n",
    "    \n",
    "    # Filter out functions without arguments\n",
    "    if \"function()\" in code or \"() =>\" in code:\n",
    "        return False\n",
    "\n",
    "    # filter out bad substrings\n",
    "    lower = code.lower()\n",
    "    for word in BAD_SUBSTRINGS:\n",
    "        if word in lower:\n",
    "            return False\n",
    "\n",
    "    for b in all_bench:\n",
    "        if b in code:  # contaminated sample!\n",
    "            return False\n",
    "\n",
    "    # too many lines of code -- say 150\n",
    "    lines = code.split(\"\\n\")\n",
    "    if len(lines) > 150:\n",
    "        return False\n",
    "\n",
    "    # filter functions which don't have an argument\n",
    "    # 1. find first def statement in lines\n",
    "    # 2. check if contains ():\n",
    "    for line in lines:\n",
    "        if line.startswith(\"def \") and \"():\" in line:\n",
    "            return False\n",
    "\n",
    "    # filter out functions with no return statement\n",
    "    parser = make_parser()\n",
    "    if not does_have_return(code, parser=parser):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        tree = global_parser.parse(code_bytes)\n",
    "        block, _ = FN_BLOCK_QUERY.captures(tree.root_node)[0]\n",
    "\n",
    "        # get the docstring, filter if not a docstring\n",
    "        exp = block.children[0]\n",
    "        if not exp.type == 'expression_statement' and not exp.children[0].type == 'string':\n",
    "            return False\n",
    "\n",
    "        docstring = exp.children[0]\n",
    "        docstring_text = docstring.text.decode('utf-8')\n",
    "        if not docstring_text.startswith('\"\"\"') and not docstring_text.endswith('\"\"\"'):\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error in filtering: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True  # all good!\n",
    "\n",
    "\n",
    "threads = os.cpu_count() - 1  # type: ignore\n",
    "dataset = dataset.filter(pre_filtering, num_proc=threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87dd56-0118-4c02-a46e-777191048250",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fe5b8-dbd1-4d93-96ca-46186b36bbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bb5f5-d5d8-468f-923e-8e8c94b99e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(f\"../../../StarCoder\", dtype=auto_dtype(),\n",
    "            gpu_memory_utilization=0.95, tensor_parallel_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caba169-b4ca-49f7-9912-b163e0a08deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188670a-403c-4e47-b230-22a3896fe9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Now running stage 3 filtering on {len(dataset)} examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ecf24-2e5a-4201-ba6c-cfcae19a9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unindent(s):\n",
    "    lines = s.splitlines()\n",
    "    non_blank_lines = [line for line in lines if line.strip()]\n",
    "    min_indent = min(len(line) - len(line.lstrip())\n",
    "                     for line in non_blank_lines) if non_blank_lines else 0\n",
    "    unindented_lines = [line[min_indent:] if len(\n",
    "        line) >= min_indent else line for line in lines]\n",
    "    return '\\n'.join(unindented_lines)\n",
    "\n",
    "\n",
    "def py_extract_docstring(code):\n",
    "    first_doc = code.find('\"\"\"')\n",
    "    assert first_doc != -1\n",
    "    first_doc = first_doc + 3\n",
    "    second_doc = code[first_doc+1:].find('\"\"\"')\n",
    "    assert second_doc != -1\n",
    "    second_doc = second_doc + first_doc + 1\n",
    "    doc = code[first_doc:second_doc]\n",
    "    doc = unindent(doc).strip()\n",
    "    code = code[:first_doc-3] + code[second_doc+3:]\n",
    "    return doc, code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c13f00-6712-413d-a039-5964f3e7c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = 'def dummy(): \\n    \"\"\"\\n    \"\"\"\\n pass'\n",
    "dummy_prompt = prompt_fmt(dummy)\n",
    "few_shot_toks = len(tokenizer.encode(\n",
    "    dummy_prompt)) - len(tokenizer.encode(dummy))\n",
    "print(f\"Few-shot prompt has {few_shot_toks} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a4e6e-e67a-4207-9c74-9e2674946387",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for ex in tqdm(dataset, total=len(dataset), desc=\"Generating prompts\"):\n",
    "    code = ex[\"content\"]\n",
    "    toks = len(tokenizer.encode(code)) + few_shot_toks\n",
    "    if toks > 16380:\n",
    "        print(f\"Skipping example with {toks} tokens\")\n",
    "        # to skip, just add dummy prompt\n",
    "        prompts.append(dummy_prompt)\n",
    "        continue\n",
    "    p = prompt_fmt(code)\n",
    "    prompts.append(p)\n",
    "\n",
    "responses = []\n",
    "for chunk in tqdm(chunkify(prompts, 512), desc=\"Generating responses\"):\n",
    "    outs = model.generate(chunk, SamplingParams(\n",
    "        temperature=0.0, stop=\"\\n\", max_tokens=5))\n",
    "    contents = [o.outputs[0].text for o in outs]\n",
    "    for c in contents:\n",
    "        yes_count = c.lower().count(\"yes\")\n",
    "        no_count = c.lower().count(\"no\")\n",
    "        if yes_count > no_count:\n",
    "            responses.append(True)\n",
    "        elif yes_count < no_count:\n",
    "            responses.append(False)\n",
    "        else:\n",
    "            # default to No\n",
    "            responses.append(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e6021-71c7-449b-a54a-d6f9a54e4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02eef9-f557-4f36-adf3-0115a5c08041",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.select(range(75000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5579e1-7c33-438e-8e79-d8acab10b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da440e82-7104-4c01-99dc-b7dac8803cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = subset.filter(  # horrible hack!\n",
    "    lambda ex, i: responses[i] and \"def dummy()\" not in ex[\"content\"], with_indices=True)\n",
    "print(f\"Filtered {len(dataset) - len(new_ds)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e497d-4afe-4651-b73c-5950e57b06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.save_to_disk(\"../datasets/seed3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148b29c-1dc5-47a3-bef7-60dc19d75764",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c466f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Parameters\n",
    "folder_to_zip = '/Users/vaibhavgupta/Desktop/LLM_Training_JS/data'    # the folder you want to zip\n",
    "archive_name  = 'data'           # output ZIP will be my_archive.zip\n",
    "\n",
    "# Create the ZIP\n",
    "shutil.make_archive(archive_name, 'zip', folder_to_zip)\n",
    "\n",
    "print(f\"Created {archive_name}.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
